{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"C:/dataanalytics/python\")\n",
    "os.curdir\n",
    "\n",
    "#Configure the environment . Set this up to the directory where spark is installed\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME'] = 'C:\\\\spark'\n",
    "    \n",
    "#create a variable for our root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "#Add the following paths to the system path. Please check your installation\n",
    "#to make sure that these zip files actually exists. The names might change as\n",
    "#versions change\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.4-src.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    " \n",
    "#Initialize a spark context\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "#optionally configure spark\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"Jeffwiz\")\n",
    "\n",
    "#Initalize spark context onl runs once\n",
    "sc = SparkContext('local', conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import DataFrame \n",
    "from pyspark.sql import Column \n",
    "from pyspark.sql import Row \n",
    "from pyspark.sql import GroupedData \n",
    "from pyspark.sql import DataFrameNaFunctions \n",
    "from pyspark.sql import DataFrameStatFunctions\n",
    "from pyspark.sql import functions \n",
    "from pyspark.sql import types \n",
    "from pyspark.sql import Window \n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = SparkSession.builder.master(\"local\").appName(\"jeff\").config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.stat import ChiSquareTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.0,3.0,4.0]\n"
     ]
    }
   ],
   "source": [
    "v = Vectors.dense([1.,2.,3.,4])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,[0,1],[1.0,2.0]) (4,[0,2],[1.0,2.0])\n"
     ]
    }
   ],
   "source": [
    "s = Vectors.sparse(4,{0:1.0, 1:2.0})\n",
    "y = Vectors.sparse(4, [0,2], [1,2])\n",
    "print(s,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df1 = df.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df1, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df1, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(SparseVector(4, {0: 1.0, 3: -2.0}),),\n",
       " (DenseVector([4.0, 5.0, 0.0, 3.0]),),\n",
       " (DenseVector([6.0, 7.0, 0.0, 8.0]),),\n",
       " (SparseVector(4, {0: 9.0, 3: 1.0}),)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[features: vector]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(pearson(features)=DenseMatrix(4, 4, [1.0, 0.0556, nan, 0.4005, 0.0556, 1.0, nan, 0.9136, nan, nan, 1.0, nan, 0.4005, 0.9136, nan, 1.0], False))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = Correlation.corr(df1, \"features\").head()\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.6872892787909721,0.6822703303362126]\n",
      "degreesOfFreedom: [2, 3]\n",
      "statistics: [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df2 = df.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "r = ChiSquareTest.test(df2, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.  20. 200.]\n",
      "[1.e+00 1.e+02 1.e+04]\n",
      "[3. 3. 3.]\n",
      "[  1.  10. 100.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "mat = sc.parallelize(\n",
    "    [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([3.0, 30.0, 300.0])]\n",
    ")  # an RDD of Vectors\n",
    "\n",
    "# Compute column summary statistics.\n",
    "summary = Statistics.colStats(mat)\n",
    "print(summary.mean())  # a dense vector containing the mean value for each column\n",
    "print(summary.variance())  # column-wise variance\n",
    "print(summary.numNonzeros())# number of nonzeros in each column\n",
    "print(summary.min())#print the minimum value columwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation is: 0.8500286768773001\n",
      "[[1.         0.97888347 0.99038957]\n",
      " [0.97888347 1.         0.99774832]\n",
      " [0.99038957 0.99774832 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "seriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0])  # a series\n",
    "# seriesY must have the same number of partitions and cardinality as seriesX\n",
    "seriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0, 555.0])\n",
    "\n",
    "# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method.\n",
    "# If a method is not specified, Pearson's method will be used by default.\n",
    "print(\"Correlation is: \" + str(Statistics.corr(seriesX, seriesY, method=\"pearson\")))\n",
    "\n",
    "data = sc.parallelize(\n",
    "    [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([5.0, 33.0, 366.0])]\n",
    ")  # an RDD of Vectors\n",
    "\n",
    "# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n",
    "# If a method is not specified, Pearson's method will be used by default.\n",
    "print(Statistics.corr(data, method=\"pearson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[63] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an RDD of any key value pairs\n",
    "data = sc.parallelize([(1, 'a'), (1, 'b'), (2, 'c'), (2, 'd'), (2, 'e'), (3, 'f')])\n",
    "\n",
    "# specify the exact fraction desired from each key as a dictionary\n",
    "fractions = {1: 0.1, 2: 0.6, 3: 0.3}\n",
    "\n",
    "approxSample = data.sampleByKey(False, fractions)\n",
    "approxSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 4 \n",
      "statistic = 0.12499999999999999 \n",
      "pValue = 0.998126379239318 \n",
      "No presumption against null hypothesis: observed follows the same distribution as expected..\n",
      "\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 2 \n",
      "statistic = 0.14141414141414144 \n",
      "pValue = 0.931734784568187 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "\n",
      "Column 1:\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.0 \n",
      "pValue = 1.0 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "Column 2:\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.0 \n",
      "pValue = 1.0 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
      "Column 3:\n",
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.0 \n",
      "pValue = 1.0 \n",
      "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrices, Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "vec = Vectors.dense(0.1, 0.15, 0.2, 0.3, 0.25)  # a vector composed of the frequencies of events\n",
    "\n",
    "# compute the goodness of fit. If a second vector to test against\n",
    "# is not supplied as a parameter, the test runs against a uniform distribution.\n",
    "goodnessOfFitTestResult = Statistics.chiSqTest(vec)\n",
    "\n",
    "# summary of the test including the p-value, degrees of freedom,\n",
    "# test statistic, the method used, and the null hypothesis.\n",
    "print(\"%s\\n\" % goodnessOfFitTestResult)\n",
    "\n",
    "mat = Matrices.dense(3, 2, [1.0, 3.0, 5.0, 2.0, 4.0, 6.0])  # a contingency matrix\n",
    "\n",
    "# conduct Pearson's independence test on the input contingency matrix\n",
    "independenceTestResult = Statistics.chiSqTest(mat)\n",
    "\n",
    "# summary of the test including the p-value, degrees of freedom,\n",
    "# test statistic, the method used, and the null hypothesis.\n",
    "print(\"%s\\n\" % independenceTestResult)\n",
    "\n",
    "obs = sc.parallelize(\n",
    "    [LabeledPoint(1.0, [1.0, 0.0, 3.0]),\n",
    "     LabeledPoint(1.0, [1.0, 2.0, 0.0]),\n",
    "     LabeledPoint(1.0, [-1.0, 0.0, -0.5])]\n",
    ")  # LabeledPoint(feature, label)\n",
    "\n",
    "# The contingency table is constructed from an RDD of LabeledPoint and used to conduct\n",
    "# the independence test. Returns an array containing the ChiSquaredTestResult for every feature\n",
    "# against the label.\n",
    "featureTestResults = Statistics.chiSqTest(obs)\n",
    "\n",
    "for i, result in enumerate(featureTestResults):\n",
    "    print(\"Column %d:\\n%s\" % (i + 1, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[1., 2.],\n",
      "             [3., 4.],\n",
      "             [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "mat = Matrices.dense(3, 2, [1.0, 3.0, 5.0, 2.0, 4.0, 6.0])  \n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolmogorov-Smirnov test summary:\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.539827837277029 \n",
      "pValue = 0.06821463111921133 \n",
      "Low presumption against null hypothesis: Sample follows theoretical distribution.\n"
     ]
    }
   ],
   "source": [
    "parallelData = sc.parallelize([0.1, 0.15, 0.2, 0.3, 0.25])\n",
    "\n",
    "# run a KS test for the sample versus a standard normal distribution\n",
    "testResult = Statistics.kolmogorovSmirnovTest(parallelData, \"norm\", 0, 1)\n",
    "# summary of the test including the p-value, test statistic, and null hypothesis\n",
    "# if our p-value indicates significance, we can reject the null hypothesis\n",
    "# Note that the Scala functionality of calling Statistics.kolmogorovSmirnovTest with\n",
    "# a lambda to calculate the CDF is not made available in the Python API\n",
    "print(testResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04145944 0.07902017 0.0896292 ]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import KernelDensity\n",
    "\n",
    "# an RDD of sample data\n",
    "data = sc.parallelize([1.0, 1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 5.0, 6.0, 7.0, 8.0, 9.0, 9.0])\n",
    "\n",
    "# Construct the density estimator with the sample data and a standard deviation for the Gaussian\n",
    "# kernels\n",
    "kd = KernelDensity()\n",
    "kd.setSample(data)\n",
    "kd.setBandwidth(3.0)\n",
    "\n",
    "# Find density estimates for the given values\n",
    "densities = kd.estimate([-1.0, 2.0, 5.0])\n",
    "print(densities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
