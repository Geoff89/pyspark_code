{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"C:/dataanalytics/python\")\n",
    "os.curdir\n",
    "\n",
    "#Configure the environment . Set this up to the directory where spark is installed\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME'] = 'C:\\\\spark'\n",
    "    \n",
    "#create a variable for our root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "#Add the following paths to the system path. Please check your installation\n",
    "#to make sure that these zip files actually exists. The names might change as\n",
    "#versions change\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.6-src.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    " \n",
    "#Initialize a spark context\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "#optionally configure spark\n",
    "conf = (SparkConf().setAppName(\"V2Maestros\").setMaster(\"local[2]\").set(\"spark.executor.memory\", \"1g\"))\n",
    "\n",
    "#Initalize spark context onl runs once\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import dataframe\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import HashingTF,Tokenizer,IDF\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"featurextraction\").config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pereparing training data in form of a dataftrame\n",
    "training = spark.createDataFrame([(1.0, Vectors.dense([0.0,1.1,0.1])),\n",
    "                                  (0.0, Vectors.dense([2.0, 1.0,-1.0])),\n",
    "                                  (0.0, Vectors.dense([2.0, 1.3,1.0])),\n",
    "                                  (1.0, Vectors.dense([0.0,1.2,-0.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a logistic regression model.This instance is an estimator\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisticregression parameter :\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print out the parameters, documentation and any default values\n",
    "print(\"Logisticregression parameter :\\n\" + lr.explainParams() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learn the logistice regression model\n",
    "model1 = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 was using parameters:\n",
      "{Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='featuresCol', doc='features column name'): 'features', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='regParam', doc='regularization parameter (>= 0)'): 0.01, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='labelCol', doc='label column name'): 'label', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='maxIter', doc='maximum number of iterations (>= 0)'): 10, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2}\n"
     ]
    }
   ],
   "source": [
    "#since the model is a tranfomer i.e produced by an estimator.this prints parameter (name:value) pairs\n",
    "print(\"model 1 was using parameters:\")\n",
    "print(model1.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#aternatiely we can specify parameters in form of a dictionary format\n",
    "paramMap={lr.maxIter:20}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramMap[lr.maxIter]=30 #specifying parameters overridung the previous one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramMap.update({lr.regParam:0.1,lr.threshold:0.55}) #specifying muliple parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we can now combine paramMaps which ar python dictionaries\n",
    "paramMap2={lr.probabilityCol:\"myProbability\"}#changing the output column name\n",
    "paramMapCombined = paramMap.copy()\n",
    "paramMapCombined.update(paramMap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model2 was fit using parameters\n",
      "{Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.55, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='featuresCol', doc='features column name'): 'features', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='regParam', doc='regularization parameter (>= 0)'): 0.1, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='labelCol', doc='label column name'): 'label', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'myProbability', Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='maxIter', doc='maximum number of iterations (>= 0)'): 30, Param(parent='LogisticRegression_43e7b61ebeb7f6ff7ebc', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2}\n"
     ]
    }
   ],
   "source": [
    "#learnong a new model from the paranMapCombined paraneters. Overirdesd esrliesr parameters\n",
    "model2 = lr.fit(training, paramMapCombined)\n",
    "print(\"model2 was fit using parameters\")\n",
    "print(model2.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preparing the test data set\n",
    "test = spark.createDataFrame([\n",
    "        (1.0, Vectors.dense([-1.0,1.5,1.3])),\n",
    "        (0.0, Vectors.dense([3.0,2.0,-0.1])),\n",
    "        (1.0, Vectors.dense([0.0,2.2,-1.5]))],[\"label\",\"features\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make predictions on the test data set using transformer.transform() method#logistic regression trsnaform wil only 'Feauters column\n",
    "#Nb model2.trasform outputs a \"myprobability column na,e we renamed\n",
    "prediction = model2.transform(test)\n",
    "result = prediction.select(\"features\",\"label\",\"myProbability\", \"prediction\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features=[-1.0,1.5,1.3], label=1.0 -> prob=[0.05707304171033977,0.9429269582896603], prediction=1.0\n",
      "features=[3.0,2.0,-0.1], label=0.0 -> prob=[0.9238522311704088,0.07614776882959128], prediction=0.0\n",
      "features=[0.0,2.2,-1.5], label=1.0 -> prob=[0.10972776114779119,0.8902722388522087], prediction=1.0\n"
     ]
    }
   ],
   "source": [
    "for row in result:\n",
    "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\" %(row.features,row.label,row.myProbability,row.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pipeleines\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preparing training documents\n",
    "training = spark.createDataFrame([\n",
    "        (0, \" a b c d e spark\", 1.0),\n",
    "        (1,\"b,d\", 0.0),\n",
    "        (2, \"spark f g h\",1.0),\n",
    "        (3, \"hadoop mapreduce\", 0.0)\n",
    "    ], [\"id\", \"text\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#configuring the ml pipelines of three stage:tokenizer, HAshingTF and Lr\n",
    "tokenizer =Tokenizer(inputCol=\"text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashingTF=HashingTF(inputCol=tokenizer.getOutputCol(),outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer,hashingTF,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit rthe pipeline to training ndocument\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preparing the test data set\n",
    "test = spark.createDataFrame([\n",
    "        (4, \"spark i j k\"),\n",
    "        (5, \" L m n\"),\n",
    "        (6, \"spark hadoop spark\"),\n",
    "        (7 , \"Apche hadoop\")\n",
    "    ], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making predictions on text documents and print columns of interest\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,spark i j k) --> prob=[0.3560065012749446,0.6439934987250555], prediction=1.000000\n",
      "(5, L m n) --> prob=[0.6419790574283502,0.35802094257164985], prediction=0.000000\n",
      "(6,spark hadoop spark) --> prob=[0.3821838216142933,0.6178161783857067], prediction=1.000000\n",
      "(7,Apche hadoop) --> prob=[0.9824720959475101,0.017527904052489846], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "for row in selected.collect():\n",
    "    rid,text,prob,prediction = row\n",
    "    print(\"(%d,%s) --> prob=%s, prediction=%f\" %(rid,text,str(prob),prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
