{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"C:/dataanalytics/python\")\n",
    "os.curdir\n",
    "\n",
    "#Configure the environment . Set this up to the directory where spark is installed\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME'] = 'C:\\\\spark'\n",
    "    \n",
    "#create a variable for our root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "#Add the following paths to the system path. Please check your installation\n",
    "#to make sure that these zip files actually exists. The names might change as\n",
    "#versions change\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.6-src.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"mysql-connector-java-5.1.46-bin.jar\"))\n",
    " \n",
    "#Initialize a spark context\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "#optionally configure spark\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"Jeffwiz\")\n",
    "\n",
    "#Initalize spark context onl runs once\n",
    "sc = SparkContext('local', conf=conf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Jeffwiz\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = [('Alice', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext,Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = [('Alice', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hivectx = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import DataFrame \n",
    "from pyspark.sql import Column \n",
    "from pyspark.sql import Row \n",
    "from pyspark.sql import GroupedData\n",
    "from pyspark.sql import DataFrameNaFunctions \n",
    "from pyspark.sql import DataFrameStatFunctions\n",
    "from pyspark.sql import functions \n",
    "from pyspark.sql import types \n",
    "from pyspark.sql import Window \n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = SparkSession.builder.master(\"local\").appName(\"jeff\").config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = [('Alice', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0xd2310e4668>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkSession.builder.config(conf=SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = SparkSession.builder.master(\"local\").appName(\"jeff\").config(conf=SparkConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Alice', _2=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.createDataFrame(l).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.createDataFrame(l,['name','age']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.createDataFrame(l,['name','age'], ).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.687289278791,0.682270330336]\n",
      "degreesOfFreedom: [2, 3]\n",
      "statistics: [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = s1.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://154.123.96.166:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Jeffwiz</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Jeffwiz>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = [{'name': 'Alice', 'age': 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py:331: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=1, name='Alice')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.createDataFrame(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Alice', _2=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(l)\n",
    "s1.createDataFrame(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = s1.createDataFrame(rdd, ['name', 'age'])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('name', 'age')\n",
    "person = rdd.map(lambda r: Person(*r))\n",
    "df2 = s1.createDataFrame(person)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)])\n",
    "df3 = s1.createDataFrame(rdd, schema)\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.createDataFrame(df.toPandas()).collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(0=1, 1=2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "s1.createDataFrame(pd.DataFrame([[1, 2]])).collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = sc.textFile(r'C:\\Users\\jeffnerd\\Desktop\\IRIS.csv')\n",
    "rows = iris.map(lambda line:line.split(\",\"))\n",
    "for row in rows.take(rows.count()):print(row[1],'\\t',row[2],'\\t',row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"jeff\").config(conf=SparkConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(r'C:\\Users\\jeffnerd\\Desktop\\employees.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     _corrupt_record|\n",
      "+--------------------+\n",
      "|                   {|\n",
      "|     \"Employees\" : [|\n",
      "|                   {|\n",
      "|  \"userId\":\"rirani\",|\n",
      "|\"jobTitleName\":\"D...|\n",
      "|\"firstName\":\"Romin\",|\n",
      "| \"lastName\":\"Irani\",|\n",
      "|\"preferredFullNam...|\n",
      "|\"employeeCode\":\"E1\",|\n",
      "|      \"region\":\"CA\",|\n",
      "|\"phoneNumber\":\"40...|\n",
      "|\"emailAddress\":\"r...|\n",
      "|                  },|\n",
      "|                   {|\n",
      "|  \"userId\":\"nirani\",|\n",
      "|\"jobTitleName\":\"D...|\n",
      "| \"firstName\":\"Neil\",|\n",
      "| \"lastName\":\"Irani\",|\n",
      "|\"preferredFullNam...|\n",
      "|\"employeeCode\":\"E2\",|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(r'C:\\Users\\jeffnerd\\Desktop\\IRIS.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----------+\n",
      "|SepalLength|SepalWidth|PetalLength|PetalWidth|       Name|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "|        5.1|       3.5|        1.4|       0.2|Iris-setosa|\n",
      "|        4.9|         3|        1.4|       0.2|Iris-setosa|\n",
      "|        4.7|       3.2|        1.3|       0.2|Iris-setosa|\n",
      "|        4.6|       3.1|        1.5|       0.2|Iris-setosa|\n",
      "|          5|       3.6|        1.4|       0.2|Iris-setosa|\n",
      "|        5.4|       3.9|        1.7|       0.4|Iris-setosa|\n",
      "|        4.6|       3.4|        1.4|       0.3|Iris-setosa|\n",
      "|          5|       3.4|        1.5|       0.2|Iris-setosa|\n",
      "|        4.4|       2.9|        1.4|       0.2|Iris-setosa|\n",
      "|        4.9|       3.1|        1.5|       0.1|Iris-setosa|\n",
      "|        5.4|       3.7|        1.5|       0.2|Iris-setosa|\n",
      "|        4.8|       3.4|        1.6|       0.2|Iris-setosa|\n",
      "|        4.8|         3|        1.4|       0.1|Iris-setosa|\n",
      "|        4.3|         3|        1.1|       0.1|Iris-setosa|\n",
      "|        5.8|         4|        1.2|       0.2|Iris-setosa|\n",
      "|        5.7|       4.4|        1.5|       0.4|Iris-setosa|\n",
      "|        5.4|       3.9|        1.3|       0.4|Iris-setosa|\n",
      "|        5.1|       3.5|        1.4|       0.3|Iris-setosa|\n",
      "|        5.7|       3.8|        1.7|       0.3|Iris-setosa|\n",
      "|        5.1|       3.8|        1.5|       0.3|Iris-setosa|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(r'C:\\Users\\jeffnerd\\Desktop\\IRIS.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----------+\n",
      "|SepalLength|SepalWidth|PetalLength|PetalWidth|       Name|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "|        5.1|       3.5|        1.4|       0.2|Iris-setosa|\n",
      "|        4.9|         3|        1.4|       0.2|Iris-setosa|\n",
      "|        4.7|       3.2|        1.3|       0.2|Iris-setosa|\n",
      "|        4.6|       3.1|        1.5|       0.2|Iris-setosa|\n",
      "|          5|       3.6|        1.4|       0.2|Iris-setosa|\n",
      "|        5.4|       3.9|        1.7|       0.4|Iris-setosa|\n",
      "|        4.6|       3.4|        1.4|       0.3|Iris-setosa|\n",
      "|          5|       3.4|        1.5|       0.2|Iris-setosa|\n",
      "|        4.4|       2.9|        1.4|       0.2|Iris-setosa|\n",
      "|        4.9|       3.1|        1.5|       0.1|Iris-setosa|\n",
      "|        5.4|       3.7|        1.5|       0.2|Iris-setosa|\n",
      "|        4.8|       3.4|        1.6|       0.2|Iris-setosa|\n",
      "|        4.8|         3|        1.4|       0.1|Iris-setosa|\n",
      "|        4.3|         3|        1.1|       0.1|Iris-setosa|\n",
      "|        5.8|         4|        1.2|       0.2|Iris-setosa|\n",
      "|        5.7|       4.4|        1.5|       0.4|Iris-setosa|\n",
      "|        5.4|       3.9|        1.3|       0.4|Iris-setosa|\n",
      "|        5.1|       3.5|        1.4|       0.3|Iris-setosa|\n",
      "|        5.7|       3.8|        1.7|       0.3|Iris-setosa|\n",
      "|        5.1|       3.8|        1.5|       0.3|Iris-setosa|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SepalLength', 'string'),\n",
       " ('SepalWidth', 'string'),\n",
       " ('PetalLength', 'string'),\n",
       " ('PetalWidth', 'string'),\n",
       " ('Name', 'string')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'SepalLength'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SepalLength: string (nullable = true)\n",
      " |-- SepalWidth: string (nullable = true)\n",
      " |-- PetalLength: string (nullable = true)\n",
      " |-- PetalWidth: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|SepalLength|\n",
      "+-----------+\n",
      "|        5.1|\n",
      "|        4.9|\n",
      "|        4.7|\n",
      "|        4.6|\n",
      "|          5|\n",
      "|        5.4|\n",
      "|        4.6|\n",
      "|          5|\n",
      "|        4.4|\n",
      "|        4.9|\n",
      "|        5.4|\n",
      "|        4.8|\n",
      "|        4.8|\n",
      "|        4.3|\n",
      "|        5.8|\n",
      "|        5.7|\n",
      "|        5.4|\n",
      "|        5.1|\n",
      "|        5.7|\n",
      "|        5.1|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"SepalLength\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " df1.select(df1['SepalLength'], df1['SepalWidth'], df1['PetalLength'], df1['PetalWidth]+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------------+\n",
      "|SepalLength|SepalWidth|PetalLength|(PetalWidth + 1)|\n",
      "+-----------+----------+-----------+----------------+\n",
      "|        5.1|       3.5|        1.4|             1.2|\n",
      "|        4.9|         3|        1.4|             1.2|\n",
      "|        4.7|       3.2|        1.3|             1.2|\n",
      "|        4.6|       3.1|        1.5|             1.2|\n",
      "|          5|       3.6|        1.4|             1.2|\n",
      "|        5.4|       3.9|        1.7|             1.4|\n",
      "|        4.6|       3.4|        1.4|             1.3|\n",
      "|          5|       3.4|        1.5|             1.2|\n",
      "|        4.4|       2.9|        1.4|             1.2|\n",
      "|        4.9|       3.1|        1.5|             1.1|\n",
      "|        5.4|       3.7|        1.5|             1.2|\n",
      "|        4.8|       3.4|        1.6|             1.2|\n",
      "|        4.8|         3|        1.4|             1.1|\n",
      "|        4.3|         3|        1.1|             1.1|\n",
      "|        5.8|         4|        1.2|             1.2|\n",
      "|        5.7|       4.4|        1.5|             1.4|\n",
      "|        5.4|       3.9|        1.3|             1.4|\n",
      "|        5.1|       3.5|        1.4|             1.3|\n",
      "|        5.7|       3.8|        1.7|             1.3|\n",
      "|        5.1|       3.8|        1.5|             1.3|\n",
      "+-----------+----------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(df1['SepalLength'], df1['SepalWidth'], df1['PetalLength'], df1['PetalWidth']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----------+\n",
      "|SepalLength|SepalWidth|PetalLength|PetalWidth|       Name|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "|        5.1|       3.5|        1.4|       0.2|Iris-setosa|\n",
      "|        4.9|         3|        1.4|       0.2|Iris-setosa|\n",
      "|        4.7|       3.2|        1.3|       0.2|Iris-setosa|\n",
      "|        4.6|       3.1|        1.5|       0.2|Iris-setosa|\n",
      "|          5|       3.6|        1.4|       0.2|Iris-setosa|\n",
      "|        5.4|       3.9|        1.7|       0.4|Iris-setosa|\n",
      "|        4.6|       3.4|        1.4|       0.3|Iris-setosa|\n",
      "|          5|       3.4|        1.5|       0.2|Iris-setosa|\n",
      "|        4.4|       2.9|        1.4|       0.2|Iris-setosa|\n",
      "|        4.9|       3.1|        1.5|       0.1|Iris-setosa|\n",
      "|        5.4|       3.7|        1.5|       0.2|Iris-setosa|\n",
      "|        4.8|       3.4|        1.6|       0.2|Iris-setosa|\n",
      "|        4.8|         3|        1.4|       0.1|Iris-setosa|\n",
      "|        4.3|         3|        1.1|       0.1|Iris-setosa|\n",
      "|        5.8|         4|        1.2|       0.2|Iris-setosa|\n",
      "|        5.7|       4.4|        1.5|       0.4|Iris-setosa|\n",
      "|        5.4|       3.9|        1.3|       0.4|Iris-setosa|\n",
      "|        5.1|       3.5|        1.4|       0.3|Iris-setosa|\n",
      "|        5.7|       3.8|        1.7|       0.3|Iris-setosa|\n",
      "|        5.1|       3.8|        1.5|       0.3|Iris-setosa|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1['SepalLength'] > 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|SepalLength|count|\n",
      "+-----------+-----+\n",
      "|          7|    1|\n",
      "|        7.3|    1|\n",
      "|        6.1|    6|\n",
      "|        4.4|    3|\n",
      "|        4.5|    1|\n",
      "|        6.2|    4|\n",
      "|        6.5|    5|\n",
      "|        5.4|    6|\n",
      "|        4.9|    6|\n",
      "|        7.7|    4|\n",
      "|          5|   10|\n",
      "|        7.1|    1|\n",
      "|        5.7|    8|\n",
      "|        5.3|    1|\n",
      "|        4.3|    1|\n",
      "|        4.8|    5|\n",
      "|        7.4|    1|\n",
      "|          6|    6|\n",
      "|        5.6|    6|\n",
      "|        4.6|    4|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupby(\"SepalLength\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ({'TEMP': 'C:\\\\Users\\\\jeffnerd\\\\AppData\\\\Local\\\\Temp', 'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)', 'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files', 'APPDATA': 'C:\\\\Users\\\\jeffnerd\\\\AppData\\\\Roaming', 'MOZ_PLUGIN_PATH': 'C:\\\\Program Files (x86)\\\\Foxit Software\\\\Foxit Reader\\\\plugins\\\\', 'GIT_PAGER': 'cat', 'ALLUSERSPROFILE': 'C:\\\\ProgramData', 'USERPROFILE': 'C:\\\\Users\\\\jeffnerd', 'NUMBER_OF_PROCESSORS': '4', 'COMSPEC': 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline', 'SESSIONNAME': 'Console', 'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files', 'SPARK_HOME': 'C:\\\\spark', 'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 42 Stepping 7, GenuineIntel', 'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files', 'IPY_INTERRUPT_EVENT': '1284', 'OS': 'Windows_NT', 'TERM': 'xterm-color', 'TMP': 'C:\\\\Users\\\\jeffnerd\\\\AppData\\\\Local\\\\Temp', 'FP_NO_HOST_CHECK': 'NO', 'PROCESSOR_LEVEL': '6', 'USERDOMAIN': 'jeff', 'LOGONSERVER': '\\\\\\\\JEFF', 'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC', 'PAGER': 'cat', 'PSMODULEPATH': 'C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules\\\\', 'CLICOLOR': '1', 'CONDA_PREFIX': 'C:\\\\Users\\\\jeffnerd\\\\Anaconda3', 'PROCESSOR_ARCHITECTURE': 'AMD64', 'JPY_PARENT_PID': '1288', 'PROCESSOR_REVISION': '2a07', 'LOCALAPPDATA': 'C:\\\\Users\\\\jeffnerd\\\\AppData\\\\Local', 'JPY_INTERRUPT_EVENT': '1284', 'USERDOMAIN_ROAMINGPROFILE': 'jeff', 'PATH': 'C:\\\\Users\\\\jeffnerd\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Users\\\\jeffnerd\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Users\\\\jeffnerd\\\\Anaconda3;C:\\\\Users\\\\jeffnerd\\\\Anaconda3\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\jeffnerd\\\\Anaconda3\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\jeffnerd\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Users\\\\jeffnerd\\\\Anaconda3\\\\Scripts;C:\\\\Users\\\\jeffnerd\\\\Anaconda3\\\\Library\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Users\\\\jeffnerd\\\\Anaconda3;C:\\\\Users\\\\jeffnerd\\\\Anaconda3\\\\Scripts;C:\\\\Users\\\\jeffnerd\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Users\\\\jeffnerd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35-32\\\\Scripts\\\\;C:\\\\Users\\\\jeffnerd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35-32\\\\C:\\\\spark\\\\bin', 'PROGRAMW6432': 'C:\\\\Program Files', 'HOMEDRIVE': 'C:', 'PROGRAMDATA': 'C:\\\\ProgramData', 'PUBLIC': 'C:\\\\Users\\\\Public', 'HADOOP_HOME': 'C:\\\\winutils', 'SYSTEMDRIVE': 'C:', 'PROGRAMFILES': 'C:\\\\Program Files', 'USERNAME': 'jeffnerd', 'WINDIR': 'C:\\\\WINDOWS', 'COMPUTERNAME': 'JEFF', 'HOMEPATH': '\\\\Users\\\\jeffnerd', 'SYSTEMROOT': 'C:\\\\WINDOWS'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"IRIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----------+\n",
      "|SepalLength|SepalWidth|PetalLength|PetalWidth|       Name|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "|        5.1|       3.5|        1.4|       0.2|Iris-setosa|\n",
      "|        4.9|         3|        1.4|       0.2|Iris-setosa|\n",
      "|        4.7|       3.2|        1.3|       0.2|Iris-setosa|\n",
      "|        4.6|       3.1|        1.5|       0.2|Iris-setosa|\n",
      "|          5|       3.6|        1.4|       0.2|Iris-setosa|\n",
      "|        5.4|       3.9|        1.7|       0.4|Iris-setosa|\n",
      "|        4.6|       3.4|        1.4|       0.3|Iris-setosa|\n",
      "|          5|       3.4|        1.5|       0.2|Iris-setosa|\n",
      "|        4.4|       2.9|        1.4|       0.2|Iris-setosa|\n",
      "|        4.9|       3.1|        1.5|       0.1|Iris-setosa|\n",
      "|        5.4|       3.7|        1.5|       0.2|Iris-setosa|\n",
      "|        4.8|       3.4|        1.6|       0.2|Iris-setosa|\n",
      "|        4.8|         3|        1.4|       0.1|Iris-setosa|\n",
      "|        4.3|         3|        1.1|       0.1|Iris-setosa|\n",
      "|        5.8|         4|        1.2|       0.2|Iris-setosa|\n",
      "|        5.7|       4.4|        1.5|       0.4|Iris-setosa|\n",
      "|        5.4|       3.9|        1.3|       0.4|Iris-setosa|\n",
      "|        5.1|       3.5|        1.4|       0.3|Iris-setosa|\n",
      "|        5.7|       3.8|        1.7|       0.3|Iris-setosa|\n",
      "|        5.1|       3.8|        1.5|       0.3|Iris-setosa|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * from IRIS\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|sepal|petal|\n",
      "+-----+-----+\n",
      "|  5.1|  1.4|\n",
      "|  4.9|  1.4|\n",
      "|  4.7|  1.3|\n",
      "|  4.6|  1.5|\n",
      "|    5|  1.4|\n",
      "|  5.4|  1.7|\n",
      "|  4.6|  1.4|\n",
      "|    5|  1.5|\n",
      "|  4.4|  1.4|\n",
      "|  4.9|  1.5|\n",
      "|  5.4|  1.5|\n",
      "|  4.8|  1.6|\n",
      "|  4.8|  1.4|\n",
      "|  4.3|  1.1|\n",
      "|  5.8|  1.2|\n",
      "|  5.7|  1.5|\n",
      "|  5.4|  1.3|\n",
      "|  5.1|  1.4|\n",
      "|  5.7|  1.7|\n",
      "|  5.1|  1.5|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF1 = spark.sql(\"SELECT SepalLength as sepal, PetalLength as petal from IRIS where PetalWidth <3.0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.createGlobalTempView(\"IRIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----------+\n",
      "|SepalLength|SepalWidth|PetalLength|PetalWidth|       Name|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "|        5.1|       3.5|        1.4|       0.2|Iris-setosa|\n",
      "|        4.9|         3|        1.4|       0.2|Iris-setosa|\n",
      "|        4.7|       3.2|        1.3|       0.2|Iris-setosa|\n",
      "|        4.6|       3.1|        1.5|       0.2|Iris-setosa|\n",
      "|          5|       3.6|        1.4|       0.2|Iris-setosa|\n",
      "|        5.4|       3.9|        1.7|       0.4|Iris-setosa|\n",
      "|        4.6|       3.4|        1.4|       0.3|Iris-setosa|\n",
      "|          5|       3.4|        1.5|       0.2|Iris-setosa|\n",
      "|        4.4|       2.9|        1.4|       0.2|Iris-setosa|\n",
      "|        4.9|       3.1|        1.5|       0.1|Iris-setosa|\n",
      "|        5.4|       3.7|        1.5|       0.2|Iris-setosa|\n",
      "|        4.8|       3.4|        1.6|       0.2|Iris-setosa|\n",
      "|        4.8|         3|        1.4|       0.1|Iris-setosa|\n",
      "|        4.3|         3|        1.1|       0.1|Iris-setosa|\n",
      "|        5.8|         4|        1.2|       0.2|Iris-setosa|\n",
      "|        5.7|       4.4|        1.5|       0.4|Iris-setosa|\n",
      "|        5.4|       3.9|        1.3|       0.4|Iris-setosa|\n",
      "|        5.1|       3.5|        1.4|       0.3|Iris-setosa|\n",
      "|        5.7|       3.8|        1.7|       0.3|Iris-setosa|\n",
      "|        5.1|       3.8|        1.5|       0.3|Iris-setosa|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.IRIS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o79.load.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:79)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:340)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f1ed38a22fec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjdbcDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"jdbc\"\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"jdbc:mysql://localhost:3306/bank\"\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dbtable\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"branch\"\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"root\"\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"password\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Mog#67sag\"\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"driver\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"com.mysql.jdbc.Driver\"\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o79.load.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$6.apply(JDBCOptions.scala:79)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:79)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:340)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n"
     ]
    }
   ],
   "source": [
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/bank\") \\\n",
    "    .option(\"dbtable\", \"branch\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"Mog#67sag\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o135.jdbc.\n: java.sql.SQLException: No suitable driver\r\n\tat java.sql.DriverManager.getDriver(DriverManager.java:315)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:83)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:306)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:193)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3f778a2ce3a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m jdbcDF2 = spark.read     .jdbc(\"jdbc:mysql://localhost:3306/bank\", \"branch\",\n\u001b[0;32m----> 2\u001b[0;31m           properties={\"user\": \"root\", \"password\": \"Mog#67sag\"})\n\u001b[0m",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mjpredicates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mString\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjpredicates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o135.jdbc.\n: java.sql.SQLException: No suitable driver\r\n\tat java.sql.DriverManager.getDriver(DriverManager.java:315)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:83)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:306)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:193)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n"
     ]
    }
   ],
   "source": [
    "jdbcDF2 = spark.read \\\n",
    "    .jdbc(\"jdbc:mysql://localhost:3306/bank\", \"branch\",\n",
    "          properties={\"user\": \"root\", \"password\": \"Mog#67sag\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
