{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"C:/dataanalytics/python\")\n",
    "os.curdir\n",
    "\n",
    "#Configure the environment . Set this up to the directory where spark is installed\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME'] = 'C:\\\\spark'\n",
    "    \n",
    "#create a variable for our root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "#Add the following paths to the system path. Please check your installation\n",
    "#to make sure that these zip files actually exists. The names might change as\n",
    "#versions change\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.4-src.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    " \n",
    "#Initialize a spark context\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "#optionally configure spark\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"Jeffwiz\")\n",
    "\n",
    "#Initalize spark context onl runs once\n",
    "sc = SparkContext('local', conf=conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"hello hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = sentence.split() # Split sentence into a list of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'hello', 'world']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = HashingTF(10000) # Create vectors of size S = 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.feature.HashingTF at 0x6c1d5849b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10000, {3220: 1.0, 6497: 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "     LabeledPoint(0.0, [0.0, 1.0]),\n",
    "     LabeledPoint(1.0, [1.0, 0.0]),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n"
     ]
    }
   ],
   "source": [
    "lrm = LogisticRegressionWithSGD.train(sc.parallelize(data), iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict([1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict(sc.parallelize([[1.0, 0.0], [0.0, 1.0]])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.0,3.0]\n"
     ]
    }
   ],
   "source": [
    "denseVec2 = Vectors.dense([1.0, 2.0, 3.0]) # .. or you can use the Vectors class\n",
    "print(denseVec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,[0,2],[1.0,2.0])\n"
     ]
    }
   ],
   "source": [
    "sparseVec1 = Vectors.sparse(4, {0: 1.0, 2: 2.0})\n",
    "print(sparseVec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These can be passed as a dictionary or as two lists of indices and values.\n",
    "sparseVec1 = Vectors.sparse(4, {0: 1.0, 2: 2.0})\n",
    "sparseVec2 = Vectors.sparse(4, [0, 2], [1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10000, {3708: 1.0, 5532: 2.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"hello hello world\"\n",
    "words = sentence.split() # Split sentence into a list of terms\n",
    "tf = HashingTF(10000) # Create vectors of size S = 10,000\n",
    "tf.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.mllib.clustering import BisectingKMeans\n",
    "from pyspark.mllib.classification import StreamingLogisticRegressionWithSGD\n",
    "from pyspark.mllib.clustering import GaussianMixture\n",
    "from pyspark.mllib.clustering import PowerIterationClustering\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.mllib.clustering import LDA\n",
    "#one for from pyspark.mllib.evaluation BinaryClassificationMetrics\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "from pyspark.mllib.feature import ChiSqSelector\n",
    "from pyspark.mllib.feature import ElementwiseProduct\n",
    "from pyspark.mllib.fpm import PrefixSpan\n",
    "from pyspark.mllib.linalg import Matrix\n",
    "from pyspark.mllib.linalg import SparseMatrix\n",
    "from pyspark.mllib.linalg import QRDecomposition\n",
    "#for distributed matrixes check version\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "from pyspark.mllib.regression import RidgeRegressionModel\n",
    "from pyspark.mllib.regression import LassoModel\n",
    "from pyspark.mllib.regression import IsotonicRegression\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "from pyspark.mllib.util import JavaLoader\n",
    "from pyspark.mllib.util import LinearDataGenerator\n",
    "from pyspark.mllib.util import Loader\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "     LabeledPoint(0.0, [0.0, 1.0]),\n",
    "     LabeledPoint(1.0, [1.0, 0.0]),\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n"
     ]
    }
   ],
   "source": [
    "lrm = LogisticRegressionWithSGD.train(sc.parallelize(data), iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict([1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict(sc.parallelize([[1.0, 0.0], [0.0, 1.0]])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " lrm.clearThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7207080310200239"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict([1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "     LabeledPoint(0.0, [0.0, 1.0]),\n",
    "     LabeledPoint(1.0, [1.0, 0.0]),\n",
    "      ]\n",
    "lrm = LogisticRegressionWithSGD.train(sc.parallelize(data), iterations=10)\n",
    "lrm.predict([0.0,1.0])\n",
    "lrm.predict([1.0,0.0])\n",
    "lrm.predict(sc.parallelize([[1.0, 0.0], [0.0, 1.0]])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ccdc46e237eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m               ]\n\u001b[1;32m      7\u001b[0m \u001b[0mlrm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionWithLBFGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlrm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mlrm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlrm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "sparse_data = [\n",
    "     LabeledPoint(0.0, Vectors.sparse(2, {0: 0.0})),\n",
    "     LabeledPoint(1.0, Vectors.sparse(2, {1: 1.0})),\n",
    "     LabeledPoint(0.0, Vectors.sparse(2, {0: 1.0})),\n",
    "     LabeledPoint(1.0, Vectors.sparse(2, {1: 2.0}))\n",
    "              ]\n",
    "lrm = LogisticRegressionWithLBFGS.train(sc.parallelize(sparse_data), iterations=10)\n",
    "lrm.predict(array([0.0, 1.0]))\n",
    "lrm.predict(array([1.0, 0.0]))\n",
    "lrm.predict(Vectors.sparse(2, {1: 1.0}))\n",
    "lrm.predict(Vectors.sparse(2, {0: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_data = [\n",
    "     LabeledPoint(0.0, Vectors.sparse(2, {0: 0.0})),\n",
    "     LabeledPoint(1.0, Vectors.sparse(2, {1: 1.0})),\n",
    "     LabeledPoint(0.0, Vectors.sparse(2, {0: 1.0})),\n",
    "     LabeledPoint(1.0, Vectors.sparse(2, {1: 2.0}))\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    }
   ],
   "source": [
    "mrm = LogisticRegressionWithSGD.train(sc.parallelize(sparse_data), iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict(Vectors.sparse(2, {1: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#multiclass data\n",
    "multi_class_data = [\n",
    "     LabeledPoint(0.0, [0.0, 1.0, 0.0]),\n",
    "     LabeledPoint(1.0, [1.0, 0.0, 0.0]),\n",
    "     LabeledPoint(2.0, [0.0, 0.0, 1.0])\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(multi_class_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'numClasses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-96c18e7fe659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionWithSGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'numClasses'"
     ]
    }
   ],
   "source": [
    "mcm = LogisticRegressionWithSGD.train(data, iterations=10, numClasses=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#principal component analysis\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U factor is:\n",
      "[-0.38829130511665644,-0.9198099362554474,-0.056387441301709175,9.313225746154785e-09,0.0]\n",
      "Singular values are: [13.029275535600473,5.368578733451684,2.5330498218813755,6.323166049206486e-08,2.0226934557075942e-08]\n",
      "V factor is:\n",
      "DenseMatrix([[-0.31278534,  0.31167136,  0.30366911,  0.8409913 , -0.07446478],\n",
      "             [-0.02980145, -0.17133211, -0.02226069,  0.14664984,  0.97352733],\n",
      "             [-0.12207248,  0.15256471, -0.95070998,  0.23828799, -0.03452092],\n",
      "             [-0.71847899, -0.68096285, -0.0172245 , -0.02094998, -0.13907533],\n",
      "             [-0.60841059,  0.62170723,  0.05606596, -0.46260933,  0.16175873]])\n",
      "[-0.5301719995198351,0.2730185511901228,-0.8027319114319463,0.0,0.0]\n",
      "Singular values are: [13.029275535600473,5.368578733451684,2.5330498218813755,6.323166049206486e-08,2.0226934557075942e-08]\n",
      "V factor is:\n",
      "DenseMatrix([[-0.31278534,  0.31167136,  0.30366911,  0.8409913 , -0.07446478],\n",
      "             [-0.02980145, -0.17133211, -0.02226069,  0.14664984,  0.97352733],\n",
      "             [-0.12207248,  0.15256471, -0.95070998,  0.23828799, -0.03452092],\n",
      "             [-0.71847899, -0.68096285, -0.0172245 , -0.02094998, -0.13907533],\n",
      "             [-0.60841059,  0.62170723,  0.05606596, -0.46260933,  0.16175873]])\n",
      "[-0.7537556058139434,0.2817987790459642,0.5936682026454339,1.4901161193847656e-08,1.4901161193847656e-08]\n",
      "Singular values are: [13.029275535600473,5.368578733451684,2.5330498218813755,6.323166049206486e-08,2.0226934557075942e-08]\n",
      "V factor is:\n",
      "DenseMatrix([[-0.31278534,  0.31167136,  0.30366911,  0.8409913 , -0.07446478],\n",
      "             [-0.02980145, -0.17133211, -0.02226069,  0.14664984,  0.97352733],\n",
      "             [-0.12207248,  0.15256471, -0.95070998,  0.23828799, -0.03452092],\n",
      "             [-0.71847899, -0.68096285, -0.0172245 , -0.02094998, -0.13907533],\n",
      "             [-0.60841059,  0.62170723,  0.05606596, -0.46260933,  0.16175873]])\n"
     ]
    }
   ],
   "source": [
    "# $example on$\n",
    "rows = sc.parallelize([\n",
    "        Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "        Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "        Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "    ])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Compute the top 5 singular values and corresponding singular vectors.\n",
    "svd = mat.computeSVD(5, computeU=True)\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix.\n",
    "# $example off$\n",
    "collected = U.rows.collect()\n",
    "print(\"U factor is:\")\n",
    "for vector in collected:\n",
    "        print(vector)\n",
    "        print(\"Singular values are: %s\" % s)\n",
    "        print(\"V factor is:\\n%s\" % V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected Row Matrix of principal component:\n",
      "[1.6485728230883807,-4.013282700516296,-5.524543751369388,0.1725834469163086]\n",
      "[-4.645104331781533,-1.1167972663619026,-5.524543751369387,0.17258344691630922]\n",
      "[-6.428880535676489,-5.337951427775355,-5.524543751369389,0.17258344691630967]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "#$example on$\n",
    "rows = sc.parallelize([\n",
    "        Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "        Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "        Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "    ])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "# Compute the top 4 principal components.\n",
    "# Principal components are stored in a local dense matrix.\n",
    "pc = mat.computePrincipalComponents(4)\n",
    "\n",
    "# Project the rows to the linear space spanned by the top 4 principal components.\n",
    "projected = mat.multiply(pc)\n",
    "# $example off$\n",
    "collected = projected.rows.collect()\n",
    "print(\"Projected Row Matrix of principal component:\")\n",
    "for vector in collected:\n",
    "        print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "autoData = sc.textFile(r\"C:\\Users\\jeffnerd\\Desktop\\spark\\auto-miles-per-gallon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C:\\Users\\jeffnerd\\Desktop\\spark\\auto-miles-per-gallon.csv MapPartitionsRDD[1] at textFile at <unknown>:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove the first line (contains headers)\n",
    "dataLines = autoData.filter(lambda x: \"CYLINDERS\" not in x)\n",
    "dataLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use default for average HP\n",
    "avgHP =sc.broadcast(80.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformToNumeric( inputStr) :\n",
    "    global avgHP\n",
    "    attList=inputStr.split(\",\")\n",
    "    \n",
    "    #Replace ? values with a normal value\n",
    "    hpValue = attList[3]\n",
    "    if hpValue == \"?\":\n",
    "        hpValue=avgHP.value\n",
    "       \n",
    "    #Filter out columns not wanted at this stage\n",
    "    values= DenseVector([ float(attList[0]), \\\n",
    "                     float(attList[1]),  \\\n",
    "                     hpValue,    \\\n",
    "                     float(attList[5]),  \\\n",
    "                     float(attList[6])\n",
    "                     ])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([18.0, 8.0, 130.0, 12.0, 70.0]),\n",
       " DenseVector([15.0, 8.0, 165.0, 11.5, 70.0]),\n",
       " DenseVector([18.0, 8.0, 150.0, 11.0, 70.0]),\n",
       " DenseVector([16.0, 8.0, 150.0, 12.0, 70.0]),\n",
       " DenseVector([17.0, 8.0, 140.0, 10.5, 70.0]),\n",
       " DenseVector([15.0, 8.0, 198.0, 10.0, 70.0]),\n",
       " DenseVector([14.0, 8.0, 220.0, 9.0, 70.0]),\n",
       " DenseVector([14.0, 8.0, 215.0, 8.5, 70.0]),\n",
       " DenseVector([14.0, 8.0, 225.0, 10.0, 70.0]),\n",
       " DenseVector([15.0, 8.0, 190.0, 8.5, 70.0]),\n",
       " DenseVector([15.0, 8.0, 170.0, 10.0, 70.0]),\n",
       " DenseVector([14.0, 8.0, 160.0, 8.0, 70.0]),\n",
       " DenseVector([15.0, 8.0, 150.0, 9.5, 70.0]),\n",
       " DenseVector([14.0, 8.0, 225.0, 10.0, 70.0]),\n",
       " DenseVector([24.0, 4.0, 95.0, 15.0, 70.0]),\n",
       " DenseVector([22.0, 6.0, 95.0, 15.5, 70.0]),\n",
       " DenseVector([18.0, 6.0, 97.0, 15.5, 70.0]),\n",
       " DenseVector([21.0, 6.0, 85.0, 16.0, 70.0]),\n",
       " DenseVector([27.0, 4.0, 88.0, 14.5, 70.0]),\n",
       " DenseVector([26.0, 4.0, 46.0, 20.5, 70.0]),\n",
       " DenseVector([25.0, 4.0, 87.0, 17.5, 70.0]),\n",
       " DenseVector([24.0, 4.0, 90.0, 14.5, 70.0]),\n",
       " DenseVector([25.0, 4.0, 95.0, 17.5, 70.0]),\n",
       " DenseVector([26.0, 4.0, 113.0, 12.5, 70.0]),\n",
       " DenseVector([21.0, 6.0, 90.0, 15.0, 70.0]),\n",
       " DenseVector([10.0, 8.0, 215.0, 14.0, 70.0]),\n",
       " DenseVector([10.0, 8.0, 200.0, 15.0, 70.0]),\n",
       " DenseVector([11.0, 8.0, 210.0, 13.5, 70.0]),\n",
       " DenseVector([9.0, 8.0, 193.0, 18.5, 70.0]),\n",
       " DenseVector([27.0, 4.0, 88.0, 14.5, 71.0]),\n",
       " DenseVector([28.0, 4.0, 90.0, 15.5, 71.0]),\n",
       " DenseVector([25.0, 4.0, 95.0, 14.0, 71.0]),\n",
       " DenseVector([25.0, 4.0, 80.0, 19.0, 71.0]),\n",
       " DenseVector([19.0, 6.0, 100.0, 13.0, 71.0]),\n",
       " DenseVector([16.0, 6.0, 105.0, 15.5, 71.0]),\n",
       " DenseVector([17.0, 6.0, 100.0, 15.5, 71.0]),\n",
       " DenseVector([19.0, 6.0, 88.0, 15.5, 71.0]),\n",
       " DenseVector([18.0, 6.0, 100.0, 15.5, 71.0]),\n",
       " DenseVector([14.0, 8.0, 165.0, 12.0, 71.0]),\n",
       " DenseVector([14.0, 8.0, 175.0, 11.5, 71.0]),\n",
       " DenseVector([14.0, 8.0, 153.0, 13.5, 71.0]),\n",
       " DenseVector([14.0, 8.0, 150.0, 13.0, 71.0]),\n",
       " DenseVector([12.0, 8.0, 180.0, 11.5, 71.0]),\n",
       " DenseVector([13.0, 8.0, 170.0, 12.0, 71.0]),\n",
       " DenseVector([13.0, 8.0, 175.0, 12.0, 71.0]),\n",
       " DenseVector([18.0, 6.0, 110.0, 13.5, 71.0]),\n",
       " DenseVector([22.0, 4.0, 72.0, 19.0, 71.0]),\n",
       " DenseVector([19.0, 6.0, 100.0, 15.0, 71.0]),\n",
       " DenseVector([18.0, 6.0, 88.0, 14.5, 71.0]),\n",
       " DenseVector([23.0, 4.0, 86.0, 14.0, 71.0]),\n",
       " DenseVector([28.0, 4.0, 90.0, 14.0, 71.0]),\n",
       " DenseVector([30.0, 4.0, 70.0, 19.5, 71.0]),\n",
       " DenseVector([30.0, 4.0, 76.0, 14.5, 71.0]),\n",
       " DenseVector([31.0, 4.0, 65.0, 19.0, 71.0]),\n",
       " DenseVector([35.0, 4.0, 69.0, 18.0, 71.0]),\n",
       " DenseVector([27.0, 4.0, 60.0, 19.0, 71.0]),\n",
       " DenseVector([26.0, 4.0, 70.0, 20.5, 71.0]),\n",
       " DenseVector([24.0, 4.0, 95.0, 15.5, 72.0]),\n",
       " DenseVector([25.0, 4.0, 80.0, 17.0, 72.0]),\n",
       " DenseVector([23.0, 4.0, 54.0, 23.5, 72.0]),\n",
       " DenseVector([20.0, 4.0, 90.0, 19.5, 72.0]),\n",
       " DenseVector([21.0, 4.0, 86.0, 16.5, 72.0]),\n",
       " DenseVector([13.0, 8.0, 165.0, 12.0, 72.0]),\n",
       " DenseVector([14.0, 8.0, 175.0, 12.0, 72.0]),\n",
       " DenseVector([15.0, 8.0, 150.0, 13.5, 72.0]),\n",
       " DenseVector([14.0, 8.0, 153.0, 13.0, 72.0]),\n",
       " DenseVector([17.0, 8.0, 150.0, 11.5, 72.0]),\n",
       " DenseVector([11.0, 8.0, 208.0, 11.0, 72.0]),\n",
       " DenseVector([13.0, 8.0, 155.0, 13.5, 72.0]),\n",
       " DenseVector([12.0, 8.0, 160.0, 13.5, 72.0]),\n",
       " DenseVector([13.0, 8.0, 190.0, 12.5, 72.0]),\n",
       " DenseVector([19.0, 3.0, 97.0, 13.5, 72.0]),\n",
       " DenseVector([15.0, 8.0, 150.0, 12.5, 72.0]),\n",
       " DenseVector([13.0, 8.0, 130.0, 14.0, 72.0]),\n",
       " DenseVector([13.0, 8.0, 140.0, 16.0, 72.0]),\n",
       " DenseVector([14.0, 8.0, 150.0, 14.0, 72.0]),\n",
       " DenseVector([18.0, 4.0, 112.0, 14.5, 72.0]),\n",
       " DenseVector([22.0, 4.0, 76.0, 18.0, 72.0]),\n",
       " DenseVector([21.0, 4.0, 87.0, 19.5, 72.0]),\n",
       " DenseVector([26.0, 4.0, 69.0, 18.0, 72.0]),\n",
       " DenseVector([22.0, 4.0, 86.0, 16.0, 72.0]),\n",
       " DenseVector([28.0, 4.0, 92.0, 17.0, 72.0]),\n",
       " DenseVector([23.0, 4.0, 97.0, 14.5, 72.0]),\n",
       " DenseVector([28.0, 4.0, 80.0, 15.0, 72.0]),\n",
       " DenseVector([27.0, 4.0, 88.0, 16.5, 72.0]),\n",
       " DenseVector([13.0, 8.0, 175.0, 13.0, 73.0]),\n",
       " DenseVector([14.0, 8.0, 150.0, 11.5, 73.0]),\n",
       " DenseVector([13.0, 8.0, 145.0, 13.0, 73.0]),\n",
       " DenseVector([14.0, 8.0, 137.0, 14.5, 73.0]),\n",
       " DenseVector([15.0, 8.0, 150.0, 12.5, 73.0]),\n",
       " DenseVector([12.0, 8.0, 198.0, 11.5, 73.0]),\n",
       " DenseVector([13.0, 8.0, 150.0, 12.0, 73.0]),\n",
       " DenseVector([13.0, 8.0, 158.0, 13.0, 73.0]),\n",
       " DenseVector([14.0, 8.0, 150.0, 14.5, 73.0]),\n",
       " DenseVector([13.0, 8.0, 215.0, 11.0, 73.0]),\n",
       " DenseVector([12.0, 8.0, 225.0, 11.0, 73.0]),\n",
       " DenseVector([13.0, 8.0, 175.0, 11.0, 73.0]),\n",
       " DenseVector([18.0, 6.0, 105.0, 16.5, 73.0]),\n",
       " DenseVector([16.0, 6.0, 100.0, 18.0, 73.0]),\n",
       " DenseVector([18.0, 6.0, 100.0, 16.0, 73.0]),\n",
       " DenseVector([18.0, 6.0, 88.0, 16.5, 73.0]),\n",
       " DenseVector([23.0, 6.0, 95.0, 16.0, 73.0]),\n",
       " DenseVector([26.0, 4.0, 46.0, 21.0, 73.0]),\n",
       " DenseVector([11.0, 8.0, 150.0, 14.0, 73.0]),\n",
       " DenseVector([12.0, 8.0, 167.0, 12.5, 73.0]),\n",
       " DenseVector([13.0, 8.0, 170.0, 13.0, 73.0]),\n",
       " DenseVector([12.0, 8.0, 180.0, 12.5, 73.0]),\n",
       " DenseVector([18.0, 6.0, 100.0, 15.0, 73.0]),\n",
       " DenseVector([20.0, 4.0, 88.0, 19.0, 73.0]),\n",
       " DenseVector([21.0, 4.0, 72.0, 19.5, 73.0]),\n",
       " DenseVector([22.0, 4.0, 94.0, 16.5, 73.0]),\n",
       " DenseVector([18.0, 3.0, 90.0, 13.5, 73.0]),\n",
       " DenseVector([19.0, 4.0, 85.0, 18.5, 73.0]),\n",
       " DenseVector([21.0, 6.0, 107.0, 14.0, 73.0]),\n",
       " DenseVector([26.0, 4.0, 90.0, 15.5, 73.0]),\n",
       " DenseVector([15.0, 8.0, 145.0, 13.0, 73.0]),\n",
       " DenseVector([16.0, 8.0, 230.0, 9.5, 73.0]),\n",
       " DenseVector([29.0, 4.0, 49.0, 19.5, 73.0]),\n",
       " DenseVector([24.0, 4.0, 75.0, 15.5, 73.0]),\n",
       " DenseVector([20.0, 4.0, 91.0, 14.0, 73.0]),\n",
       " DenseVector([19.0, 4.0, 112.0, 15.5, 73.0]),\n",
       " DenseVector([15.0, 8.0, 150.0, 11.0, 73.0]),\n",
       " DenseVector([24.0, 4.0, 110.0, 14.0, 73.0]),\n",
       " DenseVector([20.0, 6.0, 122.0, 13.5, 73.0]),\n",
       " DenseVector([11.0, 8.0, 180.0, 11.0, 73.0]),\n",
       " DenseVector([20.0, 6.0, 95.0, 16.5, 74.0]),\n",
       " DenseVector([21.0, 6.0, 80.0, 17.0, 74.0]),\n",
       " DenseVector([19.0, 6.0, 100.0, 16.0, 74.0]),\n",
       " DenseVector([15.0, 6.0, 100.0, 17.0, 74.0]),\n",
       " DenseVector([31.0, 4.0, 67.0, 19.0, 74.0]),\n",
       " DenseVector([26.0, 4.0, 80.0, 16.5, 74.0]),\n",
       " DenseVector([32.0, 4.0, 65.0, 21.0, 74.0]),\n",
       " DenseVector([25.0, 4.0, 75.0, 17.0, 74.0]),\n",
       " DenseVector([16.0, 6.0, 100.0, 17.0, 74.0]),\n",
       " DenseVector([16.0, 6.0, 110.0, 18.0, 74.0]),\n",
       " DenseVector([18.0, 6.0, 105.0, 16.5, 74.0]),\n",
       " DenseVector([16.0, 8.0, 140.0, 14.0, 74.0]),\n",
       " DenseVector([13.0, 8.0, 150.0, 14.5, 74.0]),\n",
       " DenseVector([14.0, 8.0, 150.0, 13.5, 74.0]),\n",
       " DenseVector([14.0, 8.0, 140.0, 16.0, 74.0]),\n",
       " DenseVector([14.0, 8.0, 150.0, 15.5, 74.0]),\n",
       " DenseVector([29.0, 4.0, 83.0, 16.5, 74.0]),\n",
       " DenseVector([26.0, 4.0, 67.0, 15.5, 74.0]),\n",
       " DenseVector([26.0, 4.0, 78.0, 14.5, 74.0]),\n",
       " DenseVector([31.0, 4.0, 52.0, 16.5, 74.0]),\n",
       " DenseVector([32.0, 4.0, 61.0, 19.0, 74.0]),\n",
       " DenseVector([28.0, 4.0, 75.0, 14.5, 74.0]),\n",
       " DenseVector([24.0, 4.0, 75.0, 15.5, 74.0]),\n",
       " DenseVector([26.0, 4.0, 75.0, 14.0, 74.0]),\n",
       " DenseVector([24.0, 4.0, 97.0, 15.0, 74.0]),\n",
       " DenseVector([26.0, 4.0, 93.0, 15.5, 74.0]),\n",
       " DenseVector([31.0, 4.0, 67.0, 16.0, 74.0]),\n",
       " DenseVector([19.0, 6.0, 95.0, 16.0, 75.0]),\n",
       " DenseVector([18.0, 6.0, 105.0, 16.0, 75.0]),\n",
       " DenseVector([15.0, 6.0, 72.0, 21.0, 75.0]),\n",
       " DenseVector([15.0, 6.0, 72.0, 19.5, 75.0]),\n",
       " DenseVector([16.0, 8.0, 170.0, 11.5, 75.0]),\n",
       " DenseVector([15.0, 8.0, 145.0, 14.0, 75.0]),\n",
       " DenseVector([16.0, 8.0, 150.0, 14.5, 75.0]),\n",
       " DenseVector([14.0, 8.0, 148.0, 13.5, 75.0]),\n",
       " DenseVector([17.0, 6.0, 110.0, 21.0, 75.0]),\n",
       " DenseVector([16.0, 6.0, 105.0, 18.5, 75.0]),\n",
       " DenseVector([15.0, 6.0, 110.0, 19.0, 75.0]),\n",
       " DenseVector([18.0, 6.0, 95.0, 19.0, 75.0]),\n",
       " DenseVector([21.0, 6.0, 110.0, 15.0, 75.0]),\n",
       " DenseVector([20.0, 8.0, 110.0, 13.5, 75.0]),\n",
       " DenseVector([13.0, 8.0, 129.0, 12.0, 75.0]),\n",
       " DenseVector([29.0, 4.0, 75.0, 16.0, 75.0]),\n",
       " DenseVector([23.0, 4.0, 83.0, 17.0, 75.0]),\n",
       " DenseVector([20.0, 6.0, 100.0, 16.0, 75.0]),\n",
       " DenseVector([23.0, 4.0, 78.0, 18.5, 75.0]),\n",
       " DenseVector([24.0, 4.0, 96.0, 13.5, 75.0]),\n",
       " DenseVector([25.0, 4.0, 71.0, 16.5, 75.0]),\n",
       " DenseVector([24.0, 4.0, 97.0, 17.0, 75.0]),\n",
       " DenseVector([18.0, 6.0, 97.0, 14.5, 75.0]),\n",
       " DenseVector([29.0, 4.0, 70.0, 14.0, 75.0]),\n",
       " DenseVector([19.0, 6.0, 90.0, 17.0, 75.0]),\n",
       " DenseVector([23.0, 4.0, 95.0, 15.0, 75.0]),\n",
       " DenseVector([23.0, 4.0, 88.0, 17.0, 75.0]),\n",
       " DenseVector([22.0, 4.0, 98.0, 14.5, 75.0]),\n",
       " DenseVector([25.0, 4.0, 115.0, 13.5, 75.0]),\n",
       " DenseVector([33.0, 4.0, 53.0, 17.5, 75.0]),\n",
       " DenseVector([28.0, 4.0, 86.0, 15.5, 76.0]),\n",
       " DenseVector([25.0, 4.0, 81.0, 16.9, 76.0]),\n",
       " DenseVector([25.0, 4.0, 92.0, 14.9, 76.0]),\n",
       " DenseVector([26.0, 4.0, 79.0, 17.7, 76.0]),\n",
       " DenseVector([27.0, 4.0, 83.0, 15.3, 76.0]),\n",
       " DenseVector([17.5, 8.0, 140.0, 13.0, 76.0]),\n",
       " DenseVector([16.0, 8.0, 150.0, 13.0, 76.0]),\n",
       " DenseVector([15.5, 8.0, 120.0, 13.9, 76.0]),\n",
       " DenseVector([14.5, 8.0, 152.0, 12.8, 76.0]),\n",
       " DenseVector([22.0, 6.0, 100.0, 15.4, 76.0]),\n",
       " DenseVector([22.0, 6.0, 105.0, 14.5, 76.0]),\n",
       " DenseVector([24.0, 6.0, 81.0, 17.6, 76.0]),\n",
       " DenseVector([22.5, 6.0, 90.0, 17.6, 76.0]),\n",
       " DenseVector([29.0, 4.0, 52.0, 22.2, 76.0]),\n",
       " DenseVector([24.5, 4.0, 60.0, 22.1, 76.0]),\n",
       " DenseVector([29.0, 4.0, 70.0, 14.2, 76.0]),\n",
       " DenseVector([33.0, 4.0, 53.0, 17.4, 76.0]),\n",
       " DenseVector([20.0, 6.0, 100.0, 17.7, 76.0]),\n",
       " DenseVector([18.0, 6.0, 78.0, 21.0, 76.0]),\n",
       " DenseVector([18.5, 6.0, 110.0, 16.2, 76.0]),\n",
       " DenseVector([17.5, 6.0, 95.0, 17.8, 76.0]),\n",
       " DenseVector([29.5, 4.0, 71.0, 12.2, 76.0]),\n",
       " DenseVector([32.0, 4.0, 70.0, 17.0, 76.0]),\n",
       " DenseVector([28.0, 4.0, 75.0, 16.4, 76.0]),\n",
       " DenseVector([26.5, 4.0, 72.0, 13.6, 76.0]),\n",
       " DenseVector([20.0, 4.0, 102.0, 15.7, 76.0]),\n",
       " DenseVector([13.0, 8.0, 150.0, 13.2, 76.0]),\n",
       " DenseVector([19.0, 4.0, 88.0, 21.9, 76.0]),\n",
       " DenseVector([19.0, 6.0, 108.0, 15.5, 76.0]),\n",
       " DenseVector([16.5, 6.0, 120.0, 16.7, 76.0]),\n",
       " DenseVector([16.5, 8.0, 180.0, 12.1, 76.0]),\n",
       " DenseVector([13.0, 8.0, 145.0, 12.0, 76.0]),\n",
       " DenseVector([13.0, 8.0, 130.0, 15.0, 76.0]),\n",
       " DenseVector([13.0, 8.0, 150.0, 14.0, 76.0]),\n",
       " DenseVector([31.5, 4.0, 68.0, 18.5, 77.0]),\n",
       " DenseVector([30.0, 4.0, 80.0, 14.8, 77.0]),\n",
       " DenseVector([36.0, 4.0, 58.0, 18.6, 77.0]),\n",
       " DenseVector([25.5, 4.0, 96.0, 15.5, 77.0]),\n",
       " DenseVector([33.5, 4.0, 70.0, 16.8, 77.0]),\n",
       " DenseVector([17.5, 8.0, 145.0, 12.5, 77.0]),\n",
       " DenseVector([17.0, 8.0, 110.0, 19.0, 77.0]),\n",
       " DenseVector([15.5, 8.0, 145.0, 13.7, 77.0]),\n",
       " DenseVector([15.0, 8.0, 130.0, 14.9, 77.0]),\n",
       " DenseVector([17.5, 6.0, 110.0, 16.4, 77.0]),\n",
       " DenseVector([20.5, 6.0, 105.0, 16.9, 77.0]),\n",
       " DenseVector([19.0, 6.0, 100.0, 17.7, 77.0]),\n",
       " DenseVector([18.5, 6.0, 98.0, 19.0, 77.0]),\n",
       " DenseVector([16.0, 8.0, 180.0, 11.1, 77.0]),\n",
       " DenseVector([15.5, 8.0, 170.0, 11.4, 77.0]),\n",
       " DenseVector([15.5, 8.0, 190.0, 12.2, 77.0]),\n",
       " DenseVector([16.0, 8.0, 149.0, 14.5, 77.0]),\n",
       " DenseVector([29.0, 4.0, 78.0, 14.5, 77.0]),\n",
       " DenseVector([24.5, 4.0, 88.0, 16.0, 77.0]),\n",
       " DenseVector([26.0, 4.0, 75.0, 18.2, 77.0]),\n",
       " DenseVector([25.5, 4.0, 89.0, 15.8, 77.0]),\n",
       " DenseVector([30.5, 4.0, 63.0, 17.0, 77.0]),\n",
       " DenseVector([33.5, 4.0, 83.0, 15.9, 77.0]),\n",
       " DenseVector([30.0, 4.0, 67.0, 16.4, 77.0]),\n",
       " DenseVector([30.5, 4.0, 78.0, 14.1, 77.0]),\n",
       " DenseVector([22.0, 6.0, 97.0, 14.5, 77.0]),\n",
       " DenseVector([21.5, 4.0, 110.0, 12.8, 77.0]),\n",
       " DenseVector([21.5, 3.0, 110.0, 13.5, 77.0]),\n",
       " DenseVector([43.1, 4.0, 48.0, 21.5, 78.0]),\n",
       " DenseVector([36.1, 4.0, 66.0, 14.4, 78.0]),\n",
       " DenseVector([32.8, 4.0, 52.0, 19.4, 78.0]),\n",
       " DenseVector([39.4, 4.0, 70.0, 18.6, 78.0]),\n",
       " DenseVector([36.1, 4.0, 60.0, 16.4, 78.0]),\n",
       " DenseVector([19.9, 8.0, 110.0, 15.5, 78.0]),\n",
       " DenseVector([19.4, 8.0, 140.0, 13.2, 78.0]),\n",
       " DenseVector([20.2, 8.0, 139.0, 12.8, 78.0]),\n",
       " DenseVector([19.2, 6.0, 105.0, 19.2, 78.0]),\n",
       " DenseVector([20.5, 6.0, 95.0, 18.2, 78.0]),\n",
       " DenseVector([20.2, 6.0, 85.0, 15.8, 78.0]),\n",
       " DenseVector([25.1, 4.0, 88.0, 15.4, 78.0]),\n",
       " DenseVector([20.5, 6.0, 100.0, 17.2, 78.0]),\n",
       " DenseVector([19.4, 6.0, 90.0, 17.2, 78.0]),\n",
       " DenseVector([20.6, 6.0, 105.0, 15.8, 78.0]),\n",
       " DenseVector([20.8, 6.0, 85.0, 16.7, 78.0]),\n",
       " DenseVector([18.6, 6.0, 110.0, 18.7, 78.0]),\n",
       " DenseVector([18.1, 6.0, 120.0, 15.1, 78.0]),\n",
       " DenseVector([19.2, 8.0, 145.0, 13.2, 78.0]),\n",
       " DenseVector([17.7, 6.0, 165.0, 13.4, 78.0]),\n",
       " DenseVector([18.1, 8.0, 139.0, 11.2, 78.0]),\n",
       " DenseVector([17.5, 8.0, 140.0, 13.7, 78.0]),\n",
       " DenseVector([30.0, 4.0, 68.0, 16.5, 78.0]),\n",
       " DenseVector([27.5, 4.0, 95.0, 14.2, 78.0]),\n",
       " DenseVector([27.2, 4.0, 97.0, 14.7, 78.0]),\n",
       " DenseVector([30.9, 4.0, 75.0, 14.5, 78.0]),\n",
       " DenseVector([21.1, 4.0, 95.0, 14.8, 78.0]),\n",
       " DenseVector([23.2, 4.0, 105.0, 16.7, 78.0]),\n",
       " DenseVector([23.8, 4.0, 85.0, 17.6, 78.0]),\n",
       " DenseVector([23.9, 4.0, 97.0, 14.9, 78.0]),\n",
       " DenseVector([20.3, 5.0, 103.0, 15.9, 78.0]),\n",
       " DenseVector([17.0, 6.0, 125.0, 13.6, 78.0]),\n",
       " DenseVector([21.6, 4.0, 115.0, 15.7, 78.0]),\n",
       " DenseVector([16.2, 6.0, 133.0, 15.8, 78.0]),\n",
       " DenseVector([31.5, 4.0, 71.0, 14.9, 78.0]),\n",
       " DenseVector([29.5, 4.0, 68.0, 16.6, 78.0]),\n",
       " DenseVector([21.5, 6.0, 115.0, 15.4, 79.0]),\n",
       " DenseVector([19.8, 6.0, 85.0, 18.2, 79.0]),\n",
       " DenseVector([22.3, 4.0, 88.0, 17.3, 79.0]),\n",
       " DenseVector([20.2, 6.0, 90.0, 18.2, 79.0]),\n",
       " DenseVector([20.6, 6.0, 110.0, 16.6, 79.0]),\n",
       " DenseVector([17.0, 8.0, 130.0, 15.4, 79.0]),\n",
       " DenseVector([17.6, 8.0, 129.0, 13.4, 79.0]),\n",
       " DenseVector([16.5, 8.0, 138.0, 13.2, 79.0]),\n",
       " DenseVector([18.2, 8.0, 135.0, 15.2, 79.0]),\n",
       " DenseVector([16.9, 8.0, 155.0, 14.9, 79.0]),\n",
       " DenseVector([15.5, 8.0, 142.0, 14.3, 79.0]),\n",
       " DenseVector([19.2, 8.0, 125.0, 15.0, 79.0]),\n",
       " DenseVector([18.5, 8.0, 150.0, 13.0, 79.0]),\n",
       " DenseVector([31.9, 4.0, 71.0, 14.0, 79.0]),\n",
       " DenseVector([34.1, 4.0, 65.0, 15.2, 79.0]),\n",
       " DenseVector([35.7, 4.0, 80.0, 14.4, 79.0]),\n",
       " DenseVector([27.4, 4.0, 80.0, 15.0, 79.0]),\n",
       " DenseVector([25.4, 5.0, 77.0, 20.1, 79.0]),\n",
       " DenseVector([23.0, 8.0, 125.0, 17.4, 79.0]),\n",
       " DenseVector([27.2, 4.0, 71.0, 24.8, 79.0]),\n",
       " DenseVector([23.9, 8.0, 90.0, 22.2, 79.0]),\n",
       " DenseVector([34.2, 4.0, 70.0, 13.2, 79.0]),\n",
       " DenseVector([34.5, 4.0, 70.0, 14.9, 79.0]),\n",
       " DenseVector([31.8, 4.0, 65.0, 19.2, 79.0]),\n",
       " DenseVector([37.3, 4.0, 69.0, 14.7, 79.0]),\n",
       " DenseVector([28.4, 4.0, 90.0, 16.0, 79.0]),\n",
       " DenseVector([28.8, 6.0, 115.0, 11.3, 79.0]),\n",
       " DenseVector([26.8, 6.0, 115.0, 12.9, 79.0]),\n",
       " DenseVector([33.5, 4.0, 90.0, 13.2, 79.0]),\n",
       " DenseVector([41.5, 4.0, 76.0, 14.7, 80.0]),\n",
       " DenseVector([38.1, 4.0, 60.0, 18.8, 80.0]),\n",
       " DenseVector([32.1, 4.0, 70.0, 15.5, 80.0]),\n",
       " DenseVector([37.2, 4.0, 65.0, 16.4, 80.0]),\n",
       " DenseVector([28.0, 4.0, 90.0, 16.5, 80.0]),\n",
       " DenseVector([26.4, 4.0, 88.0, 18.1, 80.0]),\n",
       " DenseVector([24.3, 4.0, 90.0, 20.1, 80.0]),\n",
       " DenseVector([19.1, 6.0, 90.0, 18.7, 80.0]),\n",
       " DenseVector([34.3, 4.0, 78.0, 15.8, 80.0]),\n",
       " DenseVector([29.8, 4.0, 90.0, 15.5, 80.0]),\n",
       " DenseVector([31.3, 4.0, 75.0, 17.5, 80.0]),\n",
       " DenseVector([37.0, 4.0, 92.0, 15.0, 80.0]),\n",
       " DenseVector([32.2, 4.0, 75.0, 15.2, 80.0]),\n",
       " DenseVector([46.6, 4.0, 65.0, 17.9, 80.0]),\n",
       " DenseVector([27.9, 4.0, 105.0, 14.4, 80.0]),\n",
       " DenseVector([40.8, 4.0, 65.0, 19.2, 80.0]),\n",
       " DenseVector([44.3, 4.0, 48.0, 21.7, 80.0]),\n",
       " DenseVector([43.4, 4.0, 48.0, 23.7, 80.0]),\n",
       " DenseVector([36.4, 5.0, 67.0, 19.9, 80.0]),\n",
       " DenseVector([30.0, 4.0, 67.0, 21.8, 80.0]),\n",
       " DenseVector([44.6, 4.0, 67.0, 13.8, 80.0]),\n",
       " DenseVector([40.9, 4.0, 80.0, 17.3, 80.0]),\n",
       " DenseVector([33.8, 4.0, 67.0, 18.0, 80.0]),\n",
       " DenseVector([29.8, 4.0, 62.0, 15.3, 80.0]),\n",
       " DenseVector([32.7, 6.0, 132.0, 11.4, 80.0]),\n",
       " DenseVector([23.7, 3.0, 100.0, 12.5, 80.0]),\n",
       " DenseVector([35.0, 4.0, 88.0, 15.1, 80.0]),\n",
       " DenseVector([23.6, 4.0, 80.0, 14.3, 80.0]),\n",
       " DenseVector([32.4, 4.0, 72.0, 17.0, 80.0]),\n",
       " DenseVector([27.2, 4.0, 84.0, 15.7, 81.0]),\n",
       " DenseVector([26.6, 4.0, 84.0, 16.4, 81.0]),\n",
       " DenseVector([25.8, 4.0, 92.0, 14.4, 81.0]),\n",
       " DenseVector([23.5, 6.0, 110.0, 12.6, 81.0]),\n",
       " DenseVector([30.0, 4.0, 84.0, 12.9, 81.0]),\n",
       " DenseVector([39.1, 4.0, 58.0, 16.9, 81.0]),\n",
       " DenseVector([39.0, 4.0, 64.0, 16.4, 81.0]),\n",
       " DenseVector([35.1, 4.0, 60.0, 16.1, 81.0]),\n",
       " DenseVector([32.3, 4.0, 67.0, 17.8, 81.0]),\n",
       " DenseVector([37.0, 4.0, 65.0, 19.4, 81.0]),\n",
       " DenseVector([37.7, 4.0, 62.0, 17.3, 81.0]),\n",
       " DenseVector([34.1, 4.0, 68.0, 16.0, 81.0]),\n",
       " DenseVector([34.7, 4.0, 63.0, 14.9, 81.0]),\n",
       " DenseVector([34.4, 4.0, 65.0, 16.2, 81.0]),\n",
       " DenseVector([29.9, 4.0, 65.0, 20.7, 81.0]),\n",
       " DenseVector([33.0, 4.0, 74.0, 14.2, 81.0]),\n",
       " DenseVector([34.5, 4.0, 80.0, 15.8, 81.0]),\n",
       " DenseVector([33.7, 4.0, 75.0, 14.4, 81.0]),\n",
       " DenseVector([32.4, 4.0, 75.0, 16.8, 81.0]),\n",
       " DenseVector([32.9, 4.0, 100.0, 14.8, 81.0]),\n",
       " DenseVector([31.6, 4.0, 74.0, 18.3, 81.0]),\n",
       " DenseVector([28.1, 4.0, 80.0, 20.4, 81.0]),\n",
       " DenseVector([30.7, 6.0, 76.0, 19.6, 81.0]),\n",
       " DenseVector([25.4, 6.0, 116.0, 12.6, 81.0]),\n",
       " DenseVector([24.2, 6.0, 120.0, 13.8, 81.0]),\n",
       " DenseVector([22.4, 6.0, 110.0, 15.8, 81.0]),\n",
       " DenseVector([26.6, 8.0, 105.0, 19.0, 81.0]),\n",
       " DenseVector([20.2, 6.0, 88.0, 17.1, 81.0]),\n",
       " DenseVector([17.6, 6.0, 85.0, 16.6, 81.0]),\n",
       " DenseVector([28.0, 4.0, 88.0, 19.6, 82.0]),\n",
       " DenseVector([27.0, 4.0, 88.0, 18.6, 82.0]),\n",
       " DenseVector([34.0, 4.0, 88.0, 18.0, 82.0]),\n",
       " DenseVector([31.0, 4.0, 85.0, 16.2, 82.0]),\n",
       " DenseVector([29.0, 4.0, 84.0, 16.0, 82.0]),\n",
       " DenseVector([27.0, 4.0, 90.0, 18.0, 82.0]),\n",
       " DenseVector([24.0, 4.0, 92.0, 16.4, 82.0]),\n",
       " DenseVector([23.0, 4.0, 80.0, 20.5, 82.0]),\n",
       " DenseVector([36.0, 4.0, 74.0, 15.3, 82.0]),\n",
       " DenseVector([37.0, 4.0, 68.0, 18.2, 82.0]),\n",
       " DenseVector([31.0, 4.0, 68.0, 17.6, 82.0]),\n",
       " DenseVector([38.0, 4.0, 63.0, 14.7, 82.0]),\n",
       " DenseVector([36.0, 4.0, 70.0, 17.3, 82.0]),\n",
       " DenseVector([36.0, 4.0, 88.0, 14.5, 82.0]),\n",
       " DenseVector([36.0, 4.0, 75.0, 14.5, 82.0]),\n",
       " DenseVector([34.0, 4.0, 70.0, 16.9, 82.0]),\n",
       " DenseVector([38.0, 4.0, 67.0, 15.0, 82.0]),\n",
       " DenseVector([32.0, 4.0, 67.0, 15.7, 82.0]),\n",
       " DenseVector([38.0, 4.0, 67.0, 16.2, 82.0]),\n",
       " DenseVector([25.0, 6.0, 110.0, 16.4, 82.0]),\n",
       " DenseVector([38.0, 6.0, 85.0, 17.0, 82.0]),\n",
       " DenseVector([26.0, 4.0, 92.0, 14.5, 82.0]),\n",
       " DenseVector([22.0, 6.0, 112.0, 14.7, 82.0]),\n",
       " DenseVector([32.0, 4.0, 96.0, 13.9, 82.0]),\n",
       " DenseVector([36.0, 4.0, 84.0, 13.0, 82.0]),\n",
       " DenseVector([27.0, 4.0, 90.0, 17.3, 82.0]),\n",
       " DenseVector([27.0, 4.0, 86.0, 15.6, 82.0]),\n",
       " DenseVector([44.0, 4.0, 52.0, 24.6, 82.0]),\n",
       " DenseVector([32.0, 4.0, 84.0, 11.6, 82.0]),\n",
       " DenseVector([28.0, 4.0, 79.0, 18.6, 82.0]),\n",
       " DenseVector([31.0, 4.0, 82.0, 19.4, 82.0])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keep only MPG, CYLINDERS, HP,ACCELERATION and MODELYEAR\n",
    "autoVectors = dataLines.map(transformToNumeric)\n",
    "autoVectors.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o267.colStats.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\linalg\\__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeColumnSummaryStatistics(RowMatrix.scala:419)\r\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:46)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.colStats(PythonMLLibAPI.scala:837)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\linalg\\__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2273aaff8d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mautoStats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mStatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolStats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mautoVectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\stat\\_statistics.py\u001b[0m in \u001b[0;36mcolStats\u001b[0;34m(rdd)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mcStats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"colStats\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mMultivariateStatisticalSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcStats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o267.colStats.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\linalg\\__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\r\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeColumnSummaryStatistics(RowMatrix.scala:419)\r\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:46)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.colStats(PythonMLLibAPI.scala:837)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\mllib\\linalg\\__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "autoStats=Statistics.colStats(autoVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 23.51457286,   5.45477387, 104.10050251,  15.56809045,\n",
       "        76.01005025])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoStats.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  61.08961077,    2.89341544, 1468.09062947,    7.60484823,\n",
       "         13.67244282])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoStats.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.,  3., 46.,  8., 70.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoStats.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 46.6,   8. , 230. ,  24.8,  82. ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoStats.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.77539629, -0.77463084,  0.42028891,  0.57926713],\n",
       "       [-0.77539629,  1.        ,  0.84275215, -0.50541949, -0.3487458 ],\n",
       "       [-0.77463084,  0.84275215,  1.        , -0.68829885, -0.41559383],\n",
       "       [ 0.42028891, -0.50541949, -0.68829885,  1.        ,  0.28813695],\n",
       "       [ 0.57926713, -0.3487458 , -0.41559383,  0.28813695,  1.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(autoVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformToLabeledPoint(inStr) :\n",
    "    lp = ( float(inStr[0]), Vectors.dense([inStr[1],inStr[2],inStr[4]]))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoLp = autoVectors.map(transformToLabeledPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n",
      "|label|        features|\n",
      "+-----+----------------+\n",
      "| 18.0|[8.0,130.0,70.0]|\n",
      "| 15.0|[8.0,165.0,70.0]|\n",
      "| 18.0|[8.0,150.0,70.0]|\n",
      "| 16.0|[8.0,150.0,70.0]|\n",
      "| 17.0|[8.0,140.0,70.0]|\n",
      "| 15.0|[8.0,198.0,70.0]|\n",
      "| 14.0|[8.0,220.0,70.0]|\n",
      "| 14.0|[8.0,215.0,70.0]|\n",
      "| 14.0|[8.0,225.0,70.0]|\n",
      "| 15.0|[8.0,190.0,70.0]|\n",
      "+-----+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autoDF = sqlContext.createDataFrame(autoLp,[\"label\", \"features\"])\n",
    "autoDF.select(\"label\",\"features\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split into training and testing data\n",
    "(trainingData, testData) = autoDF.randomSplit([0.9, 0.1])\n",
    "trainingData.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column features must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually org.apache.spark.mllib.linalg.VectorUDT@f71b0bce.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o144.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column features must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually org.apache.spark.mllib.linalg.VectorUDT@f71b0bce.\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)\r\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)\r\n\tat org.apache.spark.ml.Predictor.validateAndTransformSchema(Predictor.scala:82)\r\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:100)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3d279c67d2b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlrModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column features must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually org.apache.spark.mllib.linalg.VectorUDT@f71b0bce.'"
     ]
    }
   ],
   "source": [
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "#Predict on the test data\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.select(\"prediction\",\"label\",\"features\").show()\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"label\",metricName=\"r2\")\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = DenseVector([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 1.0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare training data from a list of (label, features) tuples.\n",
    "training = sqlContext.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 was fit using parameters: \n",
      "{}\n"
     ]
    }
   ],
   "source": [
    " #Since model1 is a Model (i.e., a transformer produced by an Estimator),\n",
    "# we can view the parameters it used during fit().\n",
    "# This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "# LogisticRegression instance.\n",
    "print(\"Model 1 was fit using parameters: \")\n",
    "print(model1.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
